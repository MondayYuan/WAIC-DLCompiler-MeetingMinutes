## 分组讨论环节问题和结论整理

下午嘉宾们分成了三组分别进行讨论，讨论结束后唐衫、金雪峰、杨军对各自小组的讨论进行了总结发言，本文档对三位嘉宾的发言进行了整理。如果有不清楚的地方，可以参考录音源文件以及录音文字稿对比阅读。

------



> 1.合理的软硬件接口抽象是什么？

Rammer V2是一种尝试，即抽象出一些tensor primitive作为接口。对硬件来说只要实现这些的primitve就够了。然后对软件来说，很大的重要意义是可以做到白盒编译，就说是以前的编译像TVM是通过搜索去做的，很难理解他的行为以及编译的时间很长，然后白盒编译的话就是说你可以一键编译，并且性能可以做分析，相当于有了cost model。



> 2.王敏捷：算子的概念有意义吗?为什么有算子这个层次？

岚松：我也不支持，XLA 里面其实一直就是200多条规则，这么长时间基本上就没增加过；算子是易用性引入的，不是高效计算引入的。一定是先打散了再优化。对于编程人员是个好东西。

敏捷：是不是大家都应该move到 xla 这个层次上？

高洋、欣然：算子是研究员的一个领域概念

雪峰：是一个 mental model，帮助研究员之间方便来进行沟通；编译阶段确实好像不是很必要；算子某种意义上就是一个对 kernel 的annotation；有的时候也碰到 xla 拼不出来的情况。

鲍翀：算子是为了让算法工程化简化。

算子是提供了一个良好的量化的边界。

总结来说，大概有几个点。一个是说算子更多的是一个研究员的一个领域概念，是用来做交流，还有一些习惯，但是在真的算子优化层面不是非常必要的。然后现在大家都在做图算融合也就是在打破边界。假设不走算子这个层次，能不能有一个比算子更往下的，但是能够相对比较统一的IR表示？目前看起来很难。



> 3.稀疏化如何支持？

尹首一：过去几年AI芯片的收益几乎都是来自于sparsity，很多顶级工作focus on input的动态稀疏。

硬件支持很难，结构化的稀疏先做；编译器方面可以提供更多支持。



> 4.大模型怎么用？

林达华：大模型资源趋向集中，形成马太效应，长期而言，可能会抑制整个领域的创新能力。

薛继龙：采用类似区块链的分布式或者联邦学习的方法，组成去中心化的算力网络，用户以类似挖矿的方式参与大模型训练，最后得到训练后的模型作为回报。



> 5.动态shape的价值有多大？如何支持？

金雪峰：这个问题是不得不支持。可能这有两个问题，一个是静态图是不是要支持动态shape，这是倒是个问题。从模型的角度，一些检测模型本来就是个动态shape，当然也可以去强制转化成一堆的静态图去做，但是这个感觉代价太大了。所以说这里这个问题是不是说，静态图里面要不要支持动态shape？我要理解一下这个问题。

唐衫：我们的问题是说，支持这个动态shape又保证性能，应该的途径是什么？还是说可能损失一些性能去更简单的识别，可能存在和性能之间的trade off。



> 6. runtime和compiler如何更合理的分工？

compiler编译出来这种binary文件以后，runtime要负责把binary给加载进来，可能会跟芯片的driver非常相关。比如driver是否开放，比如说接口是粗颗粒度还是细颗粒度，包括memory接口是不是开放出来等等。从芯片角度来说的话，要考虑runtime怎么和芯片和driver更好的一个配合。比如说芯片，现在很多加速芯片是非常大粗颗粒度的，而GPU相对来说是更细颗粒度的，现在很多生态的runtime确实也是更加适配细颗粒度，因为细颗粒度使软件层可以做更多的事情，这个时候怎么去做这个trade off是runtime需要考虑的。



> 7. 在AI系统设计里，怎么能够有一个更有效的协同设计的机制，能够使background不同的软硬件人员形成共同对话？

在硬件层面，像RISC-V简化了为一个新硬件设计ISA以及软件栈的一些工作量。在软件层面，TVM、MLIR、XLA各有特点，有没有可能也聚合形成类似RISC-V的生态？



> 8. 训练场景对于性能有多强的需要？

做AI系统的常常关注20% 到30%的对训练的提升。但是从业务来说的话，很可能百分之二三十的性能提升，不如给我更多灵活性更有帮助一些。这一点上，性能提升到什么程度，是做新的系统新的硬件需要观测的点。



> 9.现在做AI系统的力量比较分散，如何聚集大家的力量做一些事？如何形成行业共识？

可能组成一些committee，定期进行讨论总结，把大家对这个领域的理解形成一个common的可能会reuse的东西，有利于整个行业的进展。



> 10. 岚松：如何构建更好的编译器生态？

岚松：为什么一个一个 pass 的方案不是最优的，但是存活下来了？因为这是让更多人能参与进来的内容；MLIR 是一个好的起点，大家可以各自做独立的事情；这是不是最好的方案？

敏捷：是不是一定是一个直线的 pipeline？

岚松：比如 crush cabin 可以做上下层的透传

敏捷：optimization 不能太早，太早了容易让 debug 太难了，这有什么实际上的做法习惯么？

岚松：编译器有很多的 option，所以大家往往就要试各种各样的 option；

鲍翀：AI 芯片上做了很多的 option，但是就在讨论这些 option 能不能开出来？

敏捷：gcc 里面有 O2，O3，我们有没有可能达成一个共识，形成一个标准

岚松：比如 DCE 肯定可以随意开，但是有很多不一定的（尤其是跟硬件相关的），这一点评估起来很困难；比如用一个网络给一个大概的方向，然后再 fine-tune option

欣然：有些公司内部会用 AI for system 的思路去 tune option

雪峰：gcc 之所以生态很好了，是因为底层的 ISA 是乱七八糟的，所以大家就做的很好
MLIR 一个是多层（这是llvm 的缺点，业界的好的编译器都是多层的（但一般都是固定的层数）），它不是个标准，所以大家每个人写的层数不一样，可能就很难互通。

llvm 成功是来自于底层硬件已经很稳定了

现在编译器想要生态搞好了，多层 IR 一定要定好；一定要 ISA 稳定；

岚松：减少专用的硬件，尽量通过编译的方式来提高硬件复用率，需要和硬件工程师对话，控制多样性的冲动，要更标准化

敏捷：因为没用的专有硬件没用，所以芯片就不够行，然后就淘汰了这种芯片了，可能慢慢的就好了？

岚松：比如 input / output buffer 做死了，就不如做成能灵活腾挪的？

岚松：得看是不是 model 会慢慢收敛，如果一直不收敛性，那么硬件可能就一直在震荡了



总结：

编译器生态标准化或者说开放化面临的挑战分两个维度。第一个，MLIR的分层和其他编译器（如IBM的编译器）是不同的。IBM的编译器的分层是定义好的，而 MLIR它实际上是一个机制，只是提供了分层的能力，而没有具体定义各层。MLIR不是一个标准，而是一种公共的机制。第二个，硬件在AI领域的ISA太多了，难以形成标准。要把这两个地方解决好了以后，可能才会形成生态。



> 11.雪峰：整个深度学习的 workload 是收敛的还是发散的？

敏捷：业务上大概率会继续收敛下去，但是 research 估计会继续花式

岚松：举个例子，业务上的模型都是卡在 IO 上，但是 research 的模型都是卡在很后边

雪峰：大规模商用的还是很收敛的，绝大多数模型都是很收敛的。

现在前后处理部分都还没入图呢，这块都还没搞定呢

岚松：之前看过一个挺成熟的框架，当时也是各种缺算子；业务模型写的五花八门
雪峰：很多芯片是算子提供的不够

高洋：移动端遇到抖音这样的厂商，要求库集成进去要能在所有的手机上跑起来，几乎没法做；以现在的深度学习编译器似乎解决不了

欣然：binary 大小、初始化时间等等也会成为严重的瓶颈

高洋：比如两款芯片的 DDR不一样，可能性能就不一样；所有手机上都用32bit 指令；

欣然：在这种需要支持大量平台的情况下，编译器方案就会非常局限



总结：在生产场景，如CV、NLP、推荐等领域，逐渐在收敛到几个基本的backbone。在research场景，没有明显收敛趋势，但是不太care性能的话也无所谓。把这些生产场景常用的负载搞好，就能解决大量的性能的问题了。



> 12. MLPerf是否代表了workload的收敛?

workload的收敛要看backbone是否趋同，而MLPerf更多是代表了玩家的利益，这两个问题不是完全一样的。



> 13. conv/matmul之外的长尾算子如何优化？

取决于不规则算子的应用场景：research or 生产场景？



> 14. 如何达到算子表达能力的图灵完备性？

1. ONNX是否成功（tensorflow和pytorch都不待见onnx，但是硬件公司会支持ONNX，更多是商业问题而不是技术问题）；

2. Jittor的元算子；
3. 没有必要形成算子List协议（但是可以考虑如何加速这个过程，以DLPack为例，其成功原因在于：1.接口简单 2.后续迭代增加feature）；
4. 完备支持需要大量人力





> 15. 雪峰：AI框架的编程接口上能不能统一？

雪峰：为啥在 AI 领域有这么多语言，听起来一个 numpy + grad 接口就够了？在学术场景里基本上都已经是 numpy 了

敏捷：对于 R 来说最通用的语言是 Math，但是对于 Engineer 这块感觉各种难以统一

敏捷：总会遇到算子层面的问题吧

欣然：在 numpy level 的东西上已经事实上统一了，但是在其他的事情上确实很不统一；就像 programming language 一样难以统一，属于 flavor

岚松：之前用 JAX 做前端的时候，有一个好；TF 里不管是什么都在 GPU 上算，效率可能就不够快；但是 Jax 实际上的胶水代码都在 CPU 上，这可能是个挺高效的工作

鲍翀：：但是这是个异构问题，有时候传输代价可能大？

岚松：往往是一头一尾

敏捷：之前很明显缺的一个东西是 nn.Module、预处理之类的东西

欣然：预处理和模型不靠在一起就很不科学

欣然：MegEngine 做了一套 Tensor Interpreter，相当于做了一套 VM bytecode



> 16.雪峰：ONNX or MLIR？

雪峰：Mindspore 自己搞了一套，因为等不及 ONNX；ONNX 是个壳子

欣然：ONNX 经常容易变成搞不定就彻底自己搞，MLIR 至少能让这个事情大家有 common 祖先，更有可能做到渐进性严禁，也许能避免 ONNX 这种 0/1 问题

欣然：目前 MegEngine 的 Graph 已经都是 MLIR dialet 作为 graph presentation 了

敏捷：听起来 ONNX 是个 centre 模式，MLIR 可以是个更分布式的方案



> 17.欣然：训练框架未来是个更动态的还是更静态的？即是不是解释执行？

雪峰：如果是动态的话，硬件没法继续严禁，需要解决功耗等等；DSA 对于动态天生不友好。
也许可以通过一些方案，让用户可以尽量无感的做。
全静态的肯定也是有需要的，对于生产场景下，能快一倍是很有价值的，极致能效比。

欣然：比如你看 GPU 上，早起生态不够好，所以大家静态有收益，现在慢慢的生态好了就动态了；那未来会不会 DSA 上的训练框架也会慢慢的变成更动态了（更快了）

雪峰：PyTorch 在A100时代后，TensorCore 这么大，估计也会遇到问题

欣然：动态图的诉求肯定是很强的，5年后我们是不是只要搞研究就一定是 pytorch + GPU？

敏捷：本质上是 DSA 芯片未来如何在动态图上打败 GPU？

敏捷：AI 时代很重要的还是要看 customer，如何让 AI 下沉是很重要的

刁岚松：加速器的core不宜过大，否则workload各个维度可能并不能cover计算单元。

